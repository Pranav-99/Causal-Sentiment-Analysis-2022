{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eLYZWL4wXZ1",
        "outputId": "e0649edd-5498-4495-8876-cd9a8b3b4195"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/14Gdouwq1YbxwJPoIAfI-AnPNaIq3hw2L/10708 Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/10708\\ Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0QX0rv-wmLr"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import tqdm\n",
        "import random\n",
        "import pickle\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, embed_dim=128, gru_out=128, z_dim=1):\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru_out = gru_out\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.gru = nn.Sequential(\n",
        "            nn.Embedding(2000, embed_dim),\n",
        "            nn.GRU(embed_dim, gru_out, bidirectional=True, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear((2*gru_out)+z_dim, 1)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "        assert(x.shape == (z.shape[0], self.gru_out*2))\n",
        "        x = torch.cat((x, z), dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward_all_z(self, x, all_z, p_z):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "\n",
        "        output = torch.zeros((batch_size, 1)).cuda()\n",
        "        model_outputs = []\n",
        "        for p_zi, z_val in zip(p_z, all_z):\n",
        "            z_val = torch.unsqueeze(z_val, 0).repeat((batch_size, 1))\n",
        "            x_z = torch.cat((x, z_val), dim=1)\n",
        "            model_out_z = self.classifier(x_z)\n",
        "            model_out_z = torch.sigmoid(model_out_z)\n",
        "            output += p_zi.item() * model_out_z\n",
        "            model_outputs.append(model_out_z)\n",
        "\n",
        "        return output, model_outputs\n",
        "\n",
        "\n",
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, embed_dim=128, gru_out=128):\n",
        "\n",
        "        super(BasicModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru_out = gru_out\n",
        "\n",
        "        self.gru = nn.Sequential(\n",
        "            nn.Embedding(2000, embed_dim),\n",
        "            nn.GRU(embed_dim, gru_out, bidirectional=True, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear((2*gru_out), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def train(X_train, y_train, z_train, model, optimizer, criterion, num_epochs=1, batch_size=256, lr=0.001):\n",
        "      \n",
        "    model.train()\n",
        "    train_size = X_train.shape[0] \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss_meter = AverageMeter()\n",
        "        i = 0\n",
        "\n",
        "        for batch in range(0, train_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, train_size)\n",
        "\n",
        "            batch_X = X_train[start_index:end_index].cuda()\n",
        "            batch_z = z_train[start_index:end_index].cuda()\n",
        "\n",
        "            batch_y = y_train[start_index:end_index].cuda()\n",
        "\n",
        "            output = model(batch_X, batch_z)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_meter.update(loss.item(), (end_index-start_index))\n",
        "\n",
        "            if(i % 100 == 0):\n",
        "                print(\"Epoch: {}, Iter: {}, Training Loss: {}\".format(epoch, i, train_loss_meter.avg))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def predict(model, X_test, p_z, batch_size=256):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #all_z = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float).cuda()\n",
        "        all_z = torch.tensor([[0], [1]], dtype=torch.float).cuda()\n",
        "\n",
        "        test_size = X_test.shape[0]\n",
        "        test_preds = torch.zeros((test_size, ))\n",
        "\n",
        "        for batch in range(0, test_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, test_size)\n",
        "\n",
        "            batch_X = X_test[start_index:end_index].cuda()\n",
        "\n",
        "            output, all_outputs = model.forward_all_z(batch_X, all_z, p_z)\n",
        "\n",
        "            test_preds[start_index:end_index] = output.squeeze(dim=1).cpu()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return test_preds\n",
        "\n",
        "\n",
        "def train_std(X_train, y_train, model, optimizer, criterion, num_epochs=1, batch_size=256, lr=0.001):\n",
        "      \n",
        "    model.train()\n",
        "    train_size = X_train.shape[0] \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss_meter = AverageMeter()\n",
        "        i = 0\n",
        "\n",
        "        for batch in range(0, train_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, train_size)\n",
        "\n",
        "            batch_X = X_train[start_index:end_index].cuda()\n",
        "\n",
        "            batch_y = y_train[start_index:end_index].cuda()\n",
        "\n",
        "            output = model(batch_X)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_meter.update(loss.item(), (end_index-start_index))\n",
        "\n",
        "            if(i % 100 == 0):\n",
        "                print(\"Epoch: {}, Iter: {}, Training Loss: {}\".format(epoch, i, train_loss_meter.avg))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def predict_std(model, X_test, batch_size=256):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_size = X_test.shape[0]\n",
        "        test_preds = torch.zeros((test_size, ))\n",
        "\n",
        "        for batch in range(0, test_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, test_size)\n",
        "\n",
        "            batch_X = X_test[start_index:end_index].cuda()\n",
        "\n",
        "            output = torch.sigmoid(model.forward(batch_X))\n",
        "\n",
        "            test_preds[start_index:end_index] = output.squeeze(dim=1).cpu()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return test_preds\n",
        "\n",
        "\n",
        "def prepare_data(all_train_df, test_df, ind=1):\n",
        "\n",
        "    train_df = all_train_df[ind]\n",
        "    train_df = train_df.sample(frac=1)\n",
        "\n",
        "    train_df['text'] = train_df['text'].apply(lambda x: x.lower())\n",
        "    test_df['text'] = test_df['text'].apply(lambda x: x.lower())\n",
        "\n",
        "    train_df['text'] = train_df['text'].apply(lambda x: \" \".join(wordpunct_tokenize(x)))\n",
        "    test_df['text'] = test_df['text'].apply(lambda x: \" \".join(wordpunct_tokenize(x)))\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=2000, lower=True, split=' ', filters='#%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(train_df['text'].values)\n",
        "\n",
        "    X_train = tokenizer.texts_to_sequences(train_df['text'].values)\n",
        "    X_train = pad_sequences(X_train, maxlen=350)\n",
        "\n",
        "    X_test = tokenizer.texts_to_sequences(test_df['text'].values)\n",
        "    X_test = pad_sequences(X_test, maxlen=350)\n",
        "\n",
        "    return X_train, X_test, train_df, test_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    fp = open(\"data/train_dfs.pkl\", \"rb\")\n",
        "    all_train_df = pickle.load(fp)\n",
        "    fp.close()\n",
        "\n",
        "    fp = open(\"data/test_df.pkl\", \"rb\")\n",
        "    test_df = pickle.load(fp)\n",
        "    fp.close()\n",
        "\n",
        "    confounder = 'user_pop'\n",
        "\n",
        "    for dataset_index in range(1, 10):\n",
        "\n",
        "        print(\"\\n-------------\\nDataset Bias {}\\n\\n\".format(dataset_index))\n",
        "\n",
        "        X_train, X_test, train_df, test_df = prepare_data(all_train_df, test_df, dataset_index)\n",
        "\n",
        "        X_train_tensor = torch.from_numpy(X_train)\n",
        "        y_train_tensor = torch.from_numpy(train_df['label'].to_numpy()).unsqueeze(dim=1).float()\n",
        "        z_train_tensor = torch.from_numpy(train_df[confounder].to_numpy()).unsqueeze(dim=1)\n",
        "\n",
        "        X_test_tensor = torch.from_numpy(X_test)\n",
        "        y_test_tensor = torch.from_numpy(test_df['label'].to_numpy()).unsqueeze(dim=1).float()\n",
        "        z_test_tensor = torch.from_numpy(test_df[confounder].to_numpy()).unsqueeze(dim=1)\n",
        "\n",
        "        num_epochs = 1\n",
        "        batch_size = 256\n",
        "        lr = 0.001\n",
        "\n",
        "        model = Model().cuda()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train(X_train_tensor, y_train_tensor, z_train_tensor, model, optimizer, criterion, num_epochs=1, batch_size=batch_size, lr=lr)\n",
        "\n",
        "        p_z = Counter(train_df[confounder])\n",
        "        p_z = np.array([p_z[0], p_z[1]], dtype=float)\n",
        "        p_z /= p_z.sum()\n",
        "\n",
        "        pred_test = predict(model, X_test_tensor, p_z, batch_size=batch_size)\n",
        "        y_pred = np.round(pred_test.numpy())\n",
        "\n",
        "        print(\"\\nDataset Bias {}\\n\".format(dataset_index))      \n",
        "        print(\"Accuracy: {}\".format(accuracy_score(test_df['label'], y_pred)))\n",
        "\n",
        "        model_std = BasicModel().cuda()\n",
        "        optimizer_std = torch.optim.Adam(model_std.parameters(), lr=lr)\n",
        "\n",
        "        train_std(X_train_tensor, y_train_tensor, model_std, optimizer_std, criterion, num_epochs=1, batch_size=batch_size, lr=lr)\n",
        "\n",
        "        pred_test_std = predict_std(model_std, X_test_tensor, batch_size=batch_size)\n",
        "        y_pred_std = np.round(pred_test_std.numpy())\n",
        "\n",
        "        print(\"Accuracy (std): {}\".format(accuracy_score(test_df['label'], y_pred_std)))\n",
        "\n",
        "        print(\"\\n\\n-------------\\n\\n\", dataset_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QydFWA-vTHcv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# wrap batch of x, y, c -> loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoREl2ZtSSfJ"
      },
      "outputs": [],
      "source": [
        "# bi-gru model that gives feat(x)\n",
        "\n",
        "class TextFeaturizer(nn.Module):\n",
        "    def __init__(self, embed_dim=128, gru_out=32):\n",
        "\n",
        "        super(TextFeaturizer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru_out = gru_out\n",
        "        self.vocab_size = 2000\n",
        "\n",
        "        self.gru = nn.Sequential(\n",
        "            nn.Embedding(self.vocab_size, embed_dim),\n",
        "            nn.GRU(embed_dim, gru_out, bidirectional=True, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.output_dim = 2 * gru_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "        assert(x.shape == (batch_size, self.output_dim))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxVBCIcnSScz"
      },
      "outputs": [],
      "source": [
        "# model that gives mu, sigma for q_phi(z|x, y, c) (inputs: x_feat, y, c)\n",
        "\n",
        "class QPhi(nn.Module):\n",
        "    def __init__(self, text_feat_dim=64, c_dim=2, z_dim=16, hidden_dim=32):\n",
        "\n",
        "        super(TextFeaturizer, self).__init__()\n",
        "        self.text_feat_dim = text_feat_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.y_dim = 1\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            nn.Linear(self.text_feat_dim + self.c_dim + self.y_dim, self.hidden_dim),\n",
        "            nn.Linear(self.hidden_dim, 2 * self.z_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_feat, y, c):\n",
        "        xcy = torch.cat([x_feat, c, y], dim=1)\n",
        "        z_hat = self._model(xcy)\n",
        "        z_hat_mu = z_hat[:z_hat.size(1) // 2]\n",
        "        z_hat_logvar = z_hat[z_hat.size(1) // 2:]\n",
        "\n",
        "        assert z_hat_mu.size(1) == z_hat_logvar.size(1) == self.z_dim\n",
        "        return z_hat_mu, z_hat_logvar\n",
        "    \n",
        "    @staticmethod\n",
        "    def sample(mu, logvar):\n",
        "        # reparameterization\n",
        "        std = torch.exp(logvar / 2.0)\n",
        "        eps = torch.randn(std.shape, device=std.device)\n",
        "        return mu + std * eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QF2SqsUOSSav"
      },
      "outputs": [],
      "source": [
        "# model that gives p_theta(y | x, z) (inputs: feat(x), z)\n",
        "\n",
        "class PThetaY_XZ(nn.Module):\n",
        "    def __init__(self, text_feat_dim=64, z_dim=16):\n",
        "        super(PThetaY_XZ, self).__init__()\n",
        "\n",
        "        self.text_feat_dim = text_feat_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.classifier = nn.Linear(text_feat_dim + z_dim, 1)\n",
        "\n",
        "    def forward(self, x_feat, z):\n",
        "        x = torch.cat((x_feat, z), dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0B-TewxWQo2R"
      },
      "outputs": [],
      "source": [
        "# model that gives p_theta(feat(x^) | z) --> 2 linear layers on top of z\n",
        "\n",
        "class PThetaXfeat_Z(nn.Module):\n",
        "    def __init__(self, z_dim=16, text_feat_dim=64, hidden_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.text_feat_dim = text_feat_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dim),\n",
        "            nn.Linear(hidden_dim, text_feat_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_feat, z):\n",
        "        return self.model(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw47id5JQoz-"
      },
      "outputs": [],
      "source": [
        "# model that gives p_theta(c | z)\n",
        "\n",
        "class PThetaC_Z(nn.Module):\n",
        "    def __init__(self, z_dim=16, c_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "\n",
        "        self.projection = nn.Linear(z_dim, c_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.projection(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp78YHKWgWxJ"
      },
      "outputs": [],
      "source": [
        "def recon_loss(x, x_hat):\n",
        "    return ((x - x_hat) ** 2).mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5JWy7sIgWvE"
      },
      "outputs": [],
      "source": [
        "# KL loss term - uses mu, sigma from q_phi, gives kl loss wrt. N(0, I)\n",
        "\n",
        "def kl_loss(z_mu, z_logvar):\n",
        "    ## VERIFY\n",
        "    assert 0, \"verify me\"\n",
        "    return - 0.5 * (1 + z_logvar - (z_mu ** 2) - z_logvar).sum() / z_mu.size(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzR3CJAMgWs_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "10708_confounder_removal.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
