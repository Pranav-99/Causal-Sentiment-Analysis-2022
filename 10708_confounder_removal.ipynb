{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eLYZWL4wXZ1",
        "outputId": "bcb3b139-f222-46a0-a117-bf2527c34850"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/.shortcut-targets-by-id/14Gdouwq1YbxwJPoIAfI-AnPNaIq3hw2L/10708 Project\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/MyDrive/10708\\ Project/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g0QX0rv-wmLr"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import tqdm\n",
        "import random\n",
        "import pickle\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, embed_dim=128, gru_out=128, z_dim=1):\n",
        "\n",
        "        super(Model, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru_out = gru_out\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.gru = nn.Sequential(\n",
        "            nn.Embedding(2000, embed_dim),\n",
        "            nn.GRU(embed_dim, gru_out, bidirectional=True, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear((2*gru_out)+z_dim, 1)\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "        assert(x.shape == (z.shape[0], self.gru_out*2))\n",
        "        x = torch.cat((x, z), dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward_all_z(self, x, all_z, p_z):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "\n",
        "        output = torch.zeros((batch_size, 1)).cuda()\n",
        "        model_outputs = []\n",
        "        for p_zi, z_val in zip(p_z, all_z):\n",
        "            z_val = torch.unsqueeze(z_val, 0).repeat((batch_size, 1))\n",
        "            x_z = torch.cat((x, z_val), dim=1)\n",
        "            model_out_z = self.classifier(x_z)\n",
        "            model_out_z = torch.sigmoid(model_out_z)\n",
        "            output += p_zi.item() * model_out_z\n",
        "            model_outputs.append(model_out_z)\n",
        "\n",
        "        return output, model_outputs\n",
        "\n",
        "\n",
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, embed_dim=128, gru_out=128):\n",
        "\n",
        "        super(BasicModel, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru_out = gru_out\n",
        "\n",
        "        self.gru = nn.Sequential(\n",
        "            nn.Embedding(2000, embed_dim),\n",
        "            nn.GRU(embed_dim, gru_out, bidirectional=True, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear((2*gru_out), 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def train(X_train, y_train, z_train, model, optimizer, criterion, num_epochs=1, batch_size=256, lr=0.001):\n",
        "      \n",
        "    model.train()\n",
        "    train_size = X_train.shape[0] \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss_meter = AverageMeter()\n",
        "        i = 0\n",
        "\n",
        "        for batch in range(0, train_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, train_size)\n",
        "\n",
        "            batch_X = X_train[start_index:end_index].cuda()\n",
        "            batch_z = z_train[start_index:end_index].cuda()\n",
        "\n",
        "            batch_y = y_train[start_index:end_index].cuda()\n",
        "\n",
        "            output = model(batch_X, batch_z)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_meter.update(loss.item(), (end_index-start_index))\n",
        "\n",
        "            if(i % 100 == 0):\n",
        "                print(\"Epoch: {}, Iter: {}, Training Loss: {}\".format(epoch, i, train_loss_meter.avg))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def predict(model, X_test, p_z, batch_size=256):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        #all_z = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float).cuda()\n",
        "        all_z = torch.tensor([[0], [1]], dtype=torch.float).cuda()\n",
        "\n",
        "        test_size = X_test.shape[0]\n",
        "        test_preds = torch.zeros((test_size, ))\n",
        "\n",
        "        for batch in range(0, test_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, test_size)\n",
        "\n",
        "            batch_X = X_test[start_index:end_index].cuda()\n",
        "\n",
        "            output, all_outputs = model.forward_all_z(batch_X, all_z, p_z)\n",
        "\n",
        "            test_preds[start_index:end_index] = output.squeeze(dim=1).cpu()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return test_preds\n",
        "\n",
        "\n",
        "def train_std(X_train, y_train, model, optimizer, criterion, num_epochs=1, batch_size=256, lr=0.001):\n",
        "      \n",
        "    model.train()\n",
        "    train_size = X_train.shape[0] \n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        train_loss_meter = AverageMeter()\n",
        "        i = 0\n",
        "\n",
        "        for batch in range(0, train_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, train_size)\n",
        "\n",
        "            batch_X = X_train[start_index:end_index].cuda()\n",
        "\n",
        "            batch_y = y_train[start_index:end_index].cuda()\n",
        "\n",
        "            output = model(batch_X)\n",
        "\n",
        "            loss = criterion(output, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_meter.update(loss.item(), (end_index-start_index))\n",
        "\n",
        "            if(i % 100 == 0):\n",
        "                print(\"Epoch: {}, Iter: {}, Training Loss: {}\".format(epoch, i, train_loss_meter.avg))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "\n",
        "def predict_std(model, X_test, batch_size=256):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_size = X_test.shape[0]\n",
        "        test_preds = torch.zeros((test_size, ))\n",
        "\n",
        "        for batch in range(0, test_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, test_size)\n",
        "\n",
        "            batch_X = X_test[start_index:end_index].cuda()\n",
        "\n",
        "            output = torch.sigmoid(model.forward(batch_X))\n",
        "\n",
        "            test_preds[start_index:end_index] = output.squeeze(dim=1).cpu()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return test_preds\n",
        "\n",
        "\n",
        "def prepare_data(all_train_df, test_df, ind=1):\n",
        "\n",
        "    train_df = all_train_df[ind]\n",
        "    train_df = train_df.sample(frac=1)\n",
        "\n",
        "    train_df['text'] = train_df['text'].apply(lambda x: x.lower())\n",
        "    test_df['text'] = test_df['text'].apply(lambda x: x.lower())\n",
        "\n",
        "    train_df['text'] = train_df['text'].apply(lambda x: \" \".join(wordpunct_tokenize(x)))\n",
        "    test_df['text'] = test_df['text'].apply(lambda x: \" \".join(wordpunct_tokenize(x)))\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=2000, lower=True, split=' ', filters='#%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
        "    tokenizer.fit_on_texts(train_df['text'].values)\n",
        "\n",
        "    X_train = tokenizer.texts_to_sequences(train_df['text'].values)\n",
        "    X_train = pad_sequences(X_train, maxlen=350)\n",
        "\n",
        "    X_test = tokenizer.texts_to_sequences(test_df['text'].values)\n",
        "    X_test = pad_sequences(X_test, maxlen=350)\n",
        "\n",
        "    return X_train, X_test, train_df, test_df\n",
        "\n",
        "\n",
        "def main():\n",
        "    fp = open(\"data/train_dfs.pkl\", \"rb\")\n",
        "    all_train_df = pickle.load(fp)\n",
        "    fp.close()\n",
        "\n",
        "    fp = open(\"data/test_df.pkl\", \"rb\")\n",
        "    test_df = pickle.load(fp)\n",
        "    fp.close()\n",
        "\n",
        "    confounder = 'user_pop'\n",
        "\n",
        "    for dataset_index in range(1, 10):\n",
        "\n",
        "        print(\"\\n-------------\\nDataset Bias {}\\n\\n\".format(dataset_index))\n",
        "\n",
        "        X_train, X_test, train_df, test_df = prepare_data(all_train_df, test_df, dataset_index)\n",
        "\n",
        "        X_train_tensor = torch.from_numpy(X_train)\n",
        "        y_train_tensor = torch.from_numpy(train_df['label'].to_numpy()).unsqueeze(dim=1).float()\n",
        "        z_train_tensor = torch.from_numpy(train_df[confounder].to_numpy()).unsqueeze(dim=1)\n",
        "\n",
        "        X_test_tensor = torch.from_numpy(X_test)\n",
        "        y_test_tensor = torch.from_numpy(test_df['label'].to_numpy()).unsqueeze(dim=1).float()\n",
        "        z_test_tensor = torch.from_numpy(test_df[confounder].to_numpy()).unsqueeze(dim=1)\n",
        "\n",
        "        num_epochs = 1\n",
        "        batch_size = 256\n",
        "        lr = 0.001\n",
        "\n",
        "        model = Model().cuda()\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        train(X_train_tensor, y_train_tensor, z_train_tensor, model, optimizer, criterion, num_epochs=1, batch_size=batch_size, lr=lr)\n",
        "\n",
        "        p_z = Counter(train_df[confounder])\n",
        "        p_z = np.array([p_z[0], p_z[1]], dtype=float)\n",
        "        p_z /= p_z.sum()\n",
        "\n",
        "        pred_test = predict(model, X_test_tensor, p_z, batch_size=batch_size)\n",
        "        y_pred = np.round(pred_test.numpy())\n",
        "\n",
        "        print(\"\\nDataset Bias {}\\n\".format(dataset_index))      \n",
        "        print(\"Accuracy: {}\".format(accuracy_score(test_df['label'], y_pred)))\n",
        "\n",
        "        model_std = BasicModel().cuda()\n",
        "        optimizer_std = torch.optim.Adam(model_std.parameters(), lr=lr)\n",
        "\n",
        "        train_std(X_train_tensor, y_train_tensor, model_std, optimizer_std, criterion, num_epochs=1, batch_size=batch_size, lr=lr)\n",
        "\n",
        "        pred_test_std = predict_std(model_std, X_test_tensor, batch_size=batch_size)\n",
        "        y_pred_std = np.round(pred_test_std.numpy())\n",
        "\n",
        "        print(\"Accuracy (std): {}\".format(accuracy_score(test_df['label'], y_pred_std)))\n",
        "\n",
        "        print(\"\\n\\n-------------\\n\\n\", dataset_index)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QydFWA-vTHcv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# wrap batch of x, y, c -> loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eoREl2ZtSSfJ"
      },
      "outputs": [],
      "source": [
        "# bi-gru model that gives feat(x)\n",
        "\n",
        "class TextFeaturizer(nn.Module):\n",
        "    def __init__(self, embed_dim=128, gru_out=32):\n",
        "\n",
        "        super(TextFeaturizer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.gru_out = gru_out\n",
        "        self.vocab_size = 2000\n",
        "\n",
        "        self.gru = nn.Sequential(\n",
        "            nn.Embedding(self.vocab_size, embed_dim),\n",
        "            nn.GRU(embed_dim, gru_out, bidirectional=True, batch_first=True),\n",
        "        )\n",
        "\n",
        "        self.output_dim = 2 * gru_out\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = self.gru(x)[1].transpose(0, 1).contiguous().view((x.shape[0], -1))\n",
        "        assert(x.shape == (batch_size, self.output_dim))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mxVBCIcnSScz"
      },
      "outputs": [],
      "source": [
        "# model that gives mu, sigma for q_phi(z|x, y, c) (inputs: x_feat, y, c)\n",
        "\n",
        "class QPhi(nn.Module):\n",
        "    def __init__(self, text_feat_dim=64, c_dim=2, z_dim=16, hidden_dim=32):\n",
        "\n",
        "        super(TextFeaturizer, self).__init__()\n",
        "        self.text_feat_dim = text_feat_dim\n",
        "        self.c_dim = c_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.y_dim = 1\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self._model = nn.Sequential(\n",
        "            nn.Linear(self.text_feat_dim + self.c_dim + self.y_dim, self.hidden_dim),\n",
        "            nn.Linear(self.hidden_dim, 2 * self.z_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_feat, y, c):\n",
        "        xcy = torch.cat([x_feat, c, y], dim=1)\n",
        "        z_hat = self._model(xcy)\n",
        "        z_hat_mu = z_hat[:z_hat.size(1) // 2]\n",
        "        z_hat_logvar = z_hat[z_hat.size(1) // 2:]\n",
        "\n",
        "        assert z_hat_mu.size(1) == z_hat_logvar.size(1) == self.z_dim\n",
        "        return z_hat_mu, z_hat_logvar\n",
        "    \n",
        "    @staticmethod\n",
        "    def sample(mu, logvar):\n",
        "        # reparameterization\n",
        "        std = torch.exp(logvar / 2.0)\n",
        "        eps = torch.randn(std.shape, device=std.device)\n",
        "        return mu + std * eps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QF2SqsUOSSav"
      },
      "outputs": [],
      "source": [
        "# model that gives p_theta(y | x, z) (inputs: feat(x), z)\n",
        "\n",
        "class PThetaY_XZ(nn.Module):\n",
        "    def __init__(self, text_feat_dim=64, z_dim=16):\n",
        "        super(PThetaY_XZ, self).__init__()\n",
        "\n",
        "        self.text_feat_dim = text_feat_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.classifier = nn.Linear(text_feat_dim + z_dim, 1)\n",
        "\n",
        "    def forward(self, x_feat, z):\n",
        "        x = torch.cat((x_feat, z), dim=1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0B-TewxWQo2R"
      },
      "outputs": [],
      "source": [
        "# model that gives p_theta(feat(x^) | z) --> 2 linear layers on top of z\n",
        "\n",
        "class PThetaXfeat_Z(nn.Module):\n",
        "    def __init__(self, z_dim=16, text_feat_dim=64, hidden_dim=32):\n",
        "        super().__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.text_feat_dim = text_feat_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(z_dim, hidden_dim),\n",
        "            nn.Linear(hidden_dim, text_feat_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x_feat, z):\n",
        "        return self.model(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qw47id5JQoz-"
      },
      "outputs": [],
      "source": [
        "# model that gives p_theta(c | z)\n",
        "\n",
        "class PThetaC_Z(nn.Module):\n",
        "    def __init__(self, z_dim=16, c_dim=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.c_dim = c_dim\n",
        "\n",
        "        self.projection = nn.Linear(z_dim, c_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.projection(z)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cp78YHKWgWxJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from models.SCMModel import *"
      ],
      "metadata": {
        "id": "q9yQHtVBTx-u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = open(\"train_dfs.pkl\", \"rb\")\n",
        "all_train_df = pickle.load(fp)\n",
        "fp.close()\n",
        "\n",
        "fp = open(\"test_df.pkl\", \"rb\")\n",
        "test_df = pickle.load(fp)\n",
        "fp.close()"
      ],
      "metadata": {
        "id": "jMmXrwo7YCjW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confounder = ['user_pop', 'take_out']\n",
        "\n",
        "dataset_index = 1\n",
        "# for dataset_index in range(1, 10):\n",
        "if True:\n",
        "    print(\"\\n-------------\\nDataset Bias {}\\n\\n\".format(dataset_index))\n",
        "\n",
        "    X_train, X_test, train_df, test_df = prepare_data(all_train_df, test_df, dataset_index)\n",
        "\n",
        "    X_train_tensor = torch.from_numpy(X_train)\n",
        "    y_train_tensor = torch.from_numpy(train_df['label'].to_numpy()).unsqueeze(dim=1).float()\n",
        "    c_train_tensor = torch.from_numpy(train_df[confounder].to_numpy()) #.unsqueeze(dim=1)\n",
        "\n",
        "    X_test_tensor = torch.from_numpy(X_test)\n",
        "    y_test_tensor = torch.from_numpy(test_df['label'].to_numpy()).unsqueeze(dim=1).float()\n",
        "    c_test_tensor = torch.from_numpy(test_df[confounder].to_numpy()) #.unsqueeze(dim=1)\n",
        "\n",
        "    num_epochs = 1\n",
        "    batch_size = 256\n",
        "    lr = 0.001\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ll5Y2LGLYChO",
        "outputId": "5f4f3190-accd-438f-db7c-d8319d6b6220"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-------------\n",
            "Dataset Bias 1\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "O5JWy7sIgWvE"
      },
      "outputs": [],
      "source": [
        "from utils import *\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_scm(X_train, y_train, c_train, model, optimizer, num_epochs=1, batch_size=256, lr=0.001):\n",
        "\n",
        "    model.train()\n",
        "    train_size = X_train.shape[0]\n",
        "\n",
        "    train_loss_meter = AverageMeter()\n",
        "    recon_loss_meter = AverageMeter()\n",
        "    y_xz_loss_meter = AverageMeter()\n",
        "    c_z_loss_meter = AverageMeter()\n",
        "    KL_loss_meter = AverageMeter()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        i = 0\n",
        "\n",
        "        for batch in range(0, train_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, train_size)\n",
        "\n",
        "            batch_X = X_train[start_index:end_index].cuda()\n",
        "            batch_c = c_train[start_index:end_index].cuda()\n",
        "\n",
        "            batch_y = y_train[start_index:end_index].cuda()\n",
        "\n",
        "            loss, L_recon, L_y_xz, L_c_z, L_KL = model(batch_X, batch_y, batch_c, return_all_losses=True)\n",
        "\n",
        "            # loss = criterion(output, batch_y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_meter.update(loss.item(), (end_index-start_index))\n",
        "            recon_loss_meter.update(L_recon.item(), (end_index-start_index))\n",
        "            y_xz_loss_meter.update(L_y_xz.item(), (end_index-start_index))\n",
        "            c_z_loss_meter.update(L_c_z.item(), (end_index-start_index))\n",
        "            KL_loss_meter.update(L_KL.item(), (end_index-start_index))\n",
        "\n",
        "            if(i % 100 == 0):\n",
        "\n",
        "                print(\"Epoch: {}, Iter: {}, Training Loss: {}\".format(epoch, i, train_loss_meter.avg), end='')\n",
        "                # print(f\"loss, L_recon, L_y_xz, L_c_z, L_KL: {loss.item(), L_recon.item(), L_y_xz.item(), L_c_z.item(), L_KL.item()}\")\n",
        "                print(\", recon loss: {}\".format(recon_loss_meter.avg), end='')\n",
        "                print(\", y_xz loss: {}\".format(y_xz_loss_meter.avg), end='')\n",
        "                print(\", c_z loss: {}\".format(c_z_loss_meter.avg), end='')\n",
        "                print(\", KL loss: {}\".format(KL_loss_meter.avg))\n",
        "\n",
        "            i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GzR3CJAMgWs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4de60d5f-1920-4def-ed2c-785bc3d41faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Iter: 0, Training Loss: 33.777587890625, recon loss: 30.89276695251465, y_xz loss: 0.7086877822875977, c_z loss: 1.4163262844085693, KL loss: 0.7598075866699219\n",
            "Epoch: 0, Iter: 100, Training Loss: 7.42288861652412, recon loss: 5.31987461713281, y_xz loss: 0.5736661756982898, c_z loss: 1.2532500418105927, KL loss: 0.27609781153721386\n",
            "Epoch: 0, Iter: 200, Training Loss: 4.962673583433996, recon loss: 3.056297669511529, y_xz loss: 0.5089274585543581, c_z loss: 1.2133737007776897, KL loss: 0.18407476274528314\n",
            "Epoch: 0, Iter: 300, Training Loss: 3.9635143668152564, recon loss: 2.172328913380141, y_xz loss: 0.4679767080517702, c_z loss: 1.1779360783060524, KL loss: 0.14527267288181472\n",
            "Epoch: 0, Iter: 400, Training Loss: 3.4017325137321492, recon loss: 1.6849817631770845, y_xz loss: 0.4464745255479789, c_z loss: 1.1466370110499888, KL loss: 0.12363921418005688\n",
            "Epoch: 0, Iter: 500, Training Loss: 3.0369275242030738, recon loss: 1.3748498280307728, y_xz loss: 0.43350505531429057, c_z loss: 1.1194125284215886, KL loss: 0.10916011317999301\n",
            "Epoch: 0, Iter: 600, Training Loss: 2.777358732286983, recon loss: 1.1588127314238599, y_xz loss: 0.42524621515424793, c_z loss: 1.0952570432434463, KL loss: 0.09804274273196394\n",
            "Epoch: 0, Iter: 700, Training Loss: 2.5805974083178054, recon loss: 1.0003278208537805, y_xz loss: 0.4189550355395645, c_z loss: 1.0724879463286952, KL loss: 0.0888266063185034\n",
            "Epoch: 0, Iter: 800, Training Loss: 2.4266932000232844, recon loss: 0.8792343027982744, y_xz loss: 0.41451665569333995, c_z loss: 1.0518852019131406, KL loss: 0.08105704082773717\n",
            "Epoch: 0, Iter: 900, Training Loss: 2.301589986724938, recon loss: 0.7839227223243734, y_xz loss: 0.41095768846364716, c_z loss: 1.032291762505996, KL loss: 0.07441781435810964\n",
            "Epoch: 0, Iter: 1000, Training Loss: 2.197220742642939, recon loss: 0.7070801391858946, y_xz loss: 0.4077762647584959, c_z loss: 1.013693957955211, KL loss: 0.06867038057772787\n",
            "Epoch: 0, Iter: 1100, Training Loss: 2.108976325373775, recon loss: 0.6439104044368384, y_xz loss: 0.4053710904259989, c_z loss: 0.9960177204589428, KL loss: 0.06367710970941048\n",
            "Epoch: 0, Iter: 1200, Training Loss: 2.0329551916138318, recon loss: 0.5911579598226026, y_xz loss: 0.4026344624238645, c_z loss: 0.9798716675927498, KL loss: 0.05929110128413902\n",
            "Epoch: 0, Iter: 1300, Training Loss: 1.9661533570491196, recon loss: 0.5464749022681898, y_xz loss: 0.40008283534203926, c_z loss: 0.9641306201006064, KL loss: 0.0554649989603146\n",
            "Epoch: 0, Iter: 1400, Training Loss: 1.9072685491009154, recon loss: 0.5083282320267607, y_xz loss: 0.39794884312586815, c_z loss: 0.9488844360988706, KL loss: 0.052107037310631446\n",
            "Epoch: 0, Iter: 1500, Training Loss: 1.8547469311599172, recon loss: 0.475509161962262, y_xz loss: 0.3954709106528862, c_z loss: 0.9345942857581564, KL loss: 0.04917257253997658\n",
            "Epoch: 0, Iter: 1600, Training Loss: 1.8066644856588756, recon loss: 0.4471011825817998, y_xz loss: 0.39215808630287463, c_z loss: 0.9208250110928228, KL loss: 0.04658020593296925\n",
            "Epoch: 1, Iter: 0, Training Loss: 1.7650026187648382, recon loss: 0.4233190780157872, y_xz loss: 0.38881250493530717, c_z loss: 0.9085010305930967, KL loss: 0.04437000558934098\n",
            "Epoch: 1, Iter: 100, Training Loss: 1.7249851616741825, recon loss: 0.4016190447405572, y_xz loss: 0.3849600183267996, c_z loss: 0.8961215546907397, KL loss: 0.04228454418340453\n",
            "Epoch: 1, Iter: 200, Training Loss: 1.6886501722798206, recon loss: 0.38242767340805267, y_xz loss: 0.38114725867388727, c_z loss: 0.8846912132334451, KL loss: 0.040384026968977166\n",
            "Epoch: 1, Iter: 300, Training Loss: 1.6543635544593118, recon loss: 0.36530429925989066, y_xz loss: 0.37670233030821354, c_z loss: 0.8737472812734555, KL loss: 0.03860964390715174\n",
            "Epoch: 1, Iter: 400, Training Loss: 1.622373350555011, recon loss: 0.35013506472323164, y_xz loss: 0.3719439079196398, c_z loss: 0.863319954461164, KL loss: 0.03697442380341206\n",
            "Epoch: 1, Iter: 500, Training Loss: 1.5924521735196813, recon loss: 0.336395178770745, y_xz loss: 0.3670674522430365, c_z loss: 0.8535302337890455, KL loss: 0.0354593091577484\n",
            "Epoch: 1, Iter: 600, Training Loss: 1.5645456181738473, recon loss: 0.3239379209468572, y_xz loss: 0.36228849294704685, c_z loss: 0.8442666118090719, KL loss: 0.03405259295329033\n",
            "Epoch: 1, Iter: 700, Training Loss: 1.5381471559456423, recon loss: 0.312630006161105, y_xz loss: 0.35732831189947273, c_z loss: 0.8354410567580989, KL loss: 0.03274778148813408\n",
            "Epoch: 1, Iter: 800, Training Loss: 1.5136381155689687, recon loss: 0.3022904660348336, y_xz loss: 0.3524902506514493, c_z loss: 0.8273140567792002, KL loss: 0.031543342800063\n",
            "Epoch: 1, Iter: 900, Training Loss: 1.4904948509084193, recon loss: 0.292726803942421, y_xz loss: 0.34780693144109354, c_z loss: 0.81954190418289, KL loss: 0.030419211874365675\n",
            "Epoch: 1, Iter: 1000, Training Loss: 1.4684518690583002, recon loss: 0.28391972644052693, y_xz loss: 0.3431611317734112, c_z loss: 0.8119985068813685, KL loss: 0.029372504406364283\n",
            "Epoch: 1, Iter: 1100, Training Loss: 1.447661497745862, recon loss: 0.2758106778581041, y_xz loss: 0.3386825363779656, c_z loss: 0.8047771961707989, KL loss: 0.028391087854421113\n",
            "Epoch: 1, Iter: 1200, Training Loss: 1.428142133860564, recon loss: 0.2681189361944987, y_xz loss: 0.3342409498702567, c_z loss: 0.7983086731808372, KL loss: 0.02747357542570288\n",
            "Epoch: 1, Iter: 1300, Training Loss: 1.409351671230189, recon loss: 0.2609442607516217, y_xz loss: 0.3299716199318494, c_z loss: 0.7918153140943643, KL loss: 0.026620477186231672\n",
            "Epoch: 1, Iter: 1400, Training Loss: 1.3914273397935917, recon loss: 0.2542595431981832, y_xz loss: 0.32592192818495885, c_z loss: 0.7854201567976683, KL loss: 0.02582571229416671\n",
            "Epoch: 1, Iter: 1500, Training Loss: 1.3744751303462506, recon loss: 0.24801982155287822, y_xz loss: 0.32193604208974214, c_z loss: 0.7794337297488294, KL loss: 0.025085537689087\n",
            "Epoch: 1, Iter: 1600, Training Loss: 1.3579606139911522, recon loss: 0.2420907042940752, y_xz loss: 0.31796099780851056, c_z loss: 0.7735164585998869, KL loss: 0.024392454151789908\n",
            "Epoch: 2, Iter: 0, Training Loss: 1.3430173996558488, recon loss: 0.23674671974171332, y_xz loss: 0.3143313238586137, c_z loss: 0.7681587328707998, KL loss: 0.023780624159931513\n",
            "Epoch: 2, Iter: 100, Training Loss: 1.32808662964885, recon loss: 0.2314444986184803, y_xz loss: 0.31072998541229413, c_z loss: 0.762731901071925, KL loss: 0.02318024528271925\n",
            "Epoch: 2, Iter: 200, Training Loss: 1.3142239195416272, recon loss: 0.22646726796307062, y_xz loss: 0.30736628335023136, c_z loss: 0.7577776363709192, KL loss: 0.02261273259129455\n",
            "Epoch: 2, Iter: 300, Training Loss: 1.3006202468380488, recon loss: 0.22169926999231668, y_xz loss: 0.3039558943290867, c_z loss: 0.7528960100837712, KL loss: 0.022069073188502986\n",
            "Epoch: 2, Iter: 400, Training Loss: 1.287550352752349, recon loss: 0.21724055149159927, y_xz loss: 0.30057081380216916, c_z loss: 0.7481895132023801, KL loss: 0.021549475024534456\n",
            "Epoch: 2, Iter: 500, Training Loss: 1.275106227423949, recon loss: 0.21296303112131845, y_xz loss: 0.2973370079678498, c_z loss: 0.7437531050081885, KL loss: 0.021053084334833452\n",
            "Epoch: 2, Iter: 600, Training Loss: 1.2633163642779175, recon loss: 0.20891095151269554, y_xz loss: 0.29434534016893815, c_z loss: 0.739476801392438, KL loss: 0.02058327223523562\n",
            "Epoch: 2, Iter: 700, Training Loss: 1.251857461278215, recon loss: 0.2050171641798406, y_xz loss: 0.2912992651814259, c_z loss: 0.7354030978466117, KL loss: 0.0201379353413177\n",
            "Epoch: 2, Iter: 800, Training Loss: 1.2410448536252605, recon loss: 0.20131415499780575, y_xz loss: 0.2883880592990033, c_z loss: 0.7316271635976505, KL loss: 0.01971547698171342\n",
            "Epoch: 2, Iter: 900, Training Loss: 1.2306666554028018, recon loss: 0.19775208983205417, y_xz loss: 0.28564391501182035, c_z loss: 0.7279505198706846, KL loss: 0.019320132003197092\n",
            "Epoch: 2, Iter: 1000, Training Loss: 1.2205010722719603, recon loss: 0.19433970386537008, y_xz loss: 0.28289285851995366, c_z loss: 0.7243192031537082, KL loss: 0.01894930803602188\n",
            "Epoch: 2, Iter: 1100, Training Loss: 1.2107833176977725, recon loss: 0.19109323936866482, y_xz loss: 0.28029851805365197, c_z loss: 0.7207843747336722, KL loss: 0.018607186751713724\n",
            "Epoch: 2, Iter: 1200, Training Loss: 1.201570555763022, recon loss: 0.18791452636318834, y_xz loss: 0.2777320160113724, c_z loss: 0.7176336388617607, KL loss: 0.018290375846861308\n",
            "Epoch: 2, Iter: 1300, Training Loss: 1.1925285370749064, recon loss: 0.1848801101615328, y_xz loss: 0.2752601515534037, c_z loss: 0.71438999863055, KL loss: 0.017998277733181255\n",
            "Epoch: 2, Iter: 1400, Training Loss: 1.1837241675907735, recon loss: 0.1819621351536645, y_xz loss: 0.2729075671402604, c_z loss: 0.7111313345131223, KL loss: 0.01772313166918902\n",
            "Epoch: 2, Iter: 1500, Training Loss: 1.1752930591502129, recon loss: 0.1791756187067372, y_xz loss: 0.27057263980913976, c_z loss: 0.7080811898341014, KL loss: 0.017463611672310787\n",
            "Epoch: 2, Iter: 1600, Training Loss: 1.1669928190236087, recon loss: 0.17648759956252663, y_xz loss: 0.26830372491166693, c_z loss: 0.7049752951560729, KL loss: 0.01722620033599687\n",
            "Epoch: 3, Iter: 0, Training Loss: 1.1593425395258379, recon loss: 0.17397715981479836, y_xz loss: 0.2661961128695148, c_z loss: 0.7021567544917046, KL loss: 0.017012513374333314\n",
            "Epoch: 3, Iter: 100, Training Loss: 1.15156266903411, recon loss: 0.1714296156457911, y_xz loss: 0.2640757796826056, c_z loss: 0.6992623632390857, KL loss: 0.016794911411622683\n",
            "Epoch: 3, Iter: 200, Training Loss: 1.1443866138919414, recon loss: 0.16901802907216112, y_xz loss: 0.26210760127690264, c_z loss: 0.6966677335424225, KL loss: 0.016593250951619547\n",
            "Epoch: 3, Iter: 300, Training Loss: 1.1371948708499053, recon loss: 0.16666447111497287, y_xz loss: 0.2601141646766851, c_z loss: 0.6940121671532694, KL loss: 0.016404068746283737\n",
            "Epoch: 3, Iter: 400, Training Loss: 1.130189055006301, recon loss: 0.1644201163553518, y_xz loss: 0.258099514105267, c_z loss: 0.691450666780058, KL loss: 0.016218758570475597\n",
            "Epoch: 3, Iter: 500, Training Loss: 1.1234503567099126, recon loss: 0.16221411459019694, y_xz loss: 0.2561748180872693, c_z loss: 0.6890157882768096, KL loss: 0.01604563652767171\n",
            "Epoch: 3, Iter: 600, Training Loss: 1.1170480317281326, recon loss: 0.1601068139203054, y_xz loss: 0.2543884446581904, c_z loss: 0.6866682651371909, KL loss: 0.015884508714098376\n",
            "Epoch: 3, Iter: 700, Training Loss: 1.1106999838919118, recon loss: 0.1580379723401848, y_xz loss: 0.25252703019297995, c_z loss: 0.6844061730385295, KL loss: 0.01572880904619052\n",
            "Epoch: 3, Iter: 800, Training Loss: 1.1046561782494941, recon loss: 0.15603763186082273, y_xz loss: 0.25071836011789267, c_z loss: 0.6823135854979863, KL loss: 0.015586601544402008\n",
            "Epoch: 3, Iter: 900, Training Loss: 1.0988157600868753, recon loss: 0.15410582423223557, y_xz loss: 0.24900243781326187, c_z loss: 0.6802472640862238, KL loss: 0.015460234647006094\n",
            "Epoch: 3, Iter: 1000, Training Loss: 1.0930566992124748, recon loss: 0.15222814017182265, y_xz loss: 0.24728129363920937, c_z loss: 0.6782034123269323, KL loss: 0.015343853623297751\n",
            "Epoch: 3, Iter: 1100, Training Loss: 1.0874782043030005, recon loss: 0.1504201518294358, y_xz loss: 0.2456591819440598, c_z loss: 0.6761598593356785, KL loss: 0.015239011568205226\n",
            "Epoch: 3, Iter: 1200, Training Loss: 1.0822027695764094, recon loss: 0.14863861573654313, y_xz loss: 0.24405832839107344, c_z loss: 0.674341573508488, KL loss: 0.015164252315690695\n",
            "Epoch: 3, Iter: 1300, Training Loss: 1.0769957911149937, recon loss: 0.14692651509990565, y_xz loss: 0.24252836037450276, c_z loss: 0.6724248718703529, KL loss: 0.015116044142512206\n",
            "Epoch: 3, Iter: 1400, Training Loss: 1.071801252170492, recon loss: 0.14523601588004503, y_xz loss: 0.24104286319151672, c_z loss: 0.6704472233087722, KL loss: 0.015075150232917008\n",
            "Epoch: 3, Iter: 1500, Training Loss: 1.0668069551445012, recon loss: 0.14360783052370818, y_xz loss: 0.23954148104953374, c_z loss: 0.6686161478797941, KL loss: 0.01504149620595579\n",
            "Epoch: 3, Iter: 1600, Training Loss: 1.0618207595179074, recon loss: 0.1420177010752297, y_xz loss: 0.23807312112333978, c_z loss: 0.6667170686373765, KL loss: 0.015012869110506097\n"
          ]
        }
      ],
      "source": [
        "model = StructuralCausalModel(c_dim=len(confounder)).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_scm(X_train_tensor, y_train_tensor, c_train_tensor.float(), model, optimizer, num_epochs=4, batch_size=batch_size, lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sTMnhTAseDxi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EvX-q-RzeDvN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Move into SCM class\n",
        "def generate_z(scm, x, y, c):\n",
        "    x_feat = scm.text_featurizer(x)\n",
        "    z_hat_mu, z_hat_logvar = scm.q_phi(x_feat, y, c)\n",
        "    z_hat = scm.q_phi.sample(z_hat_mu, z_hat_logvar)\n",
        "    return z_hat\n"
      ],
      "metadata": {
        "id": "VA4RWDtovNte"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3PxJ0nu_vNrr"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_z_dataset(X_train, y_train, c_train, model, batch_size=1024):\n",
        "\n",
        "    model.eval()\n",
        "    train_size = X_train.shape[0]\n",
        "    z_values = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in range(0, train_size, batch_size):\n",
        "          start_index = batch\n",
        "          end_index = min(batch + batch_size, train_size)\n",
        "\n",
        "          batch_X = X_train[start_index:end_index].cuda()\n",
        "          batch_c = c_train[start_index:end_index].cuda()\n",
        "\n",
        "          batch_y = y_train[start_index:end_index].cuda()\n",
        "\n",
        "          batch_z = generate_z(model, batch_X, batch_y, batch_c).detach().cpu().numpy()\n",
        "          z_values.append(batch_z)\n",
        "\n",
        "    z_values = np.vstack(z_values)\n",
        "    return z_values"
      ],
      "metadata": {
        "id": "w3J6-ajZvzzx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_dataset_train = generate_z_dataset(X_train_tensor, y_train_tensor, c_train_tensor.float(), model)"
      ],
      "metadata": {
        "id": "usEWDp-qvzxs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_dataset_test = generate_z_dataset(X_test_tensor, y_test_tensor, c_test_tensor.float(), model)"
      ],
      "metadata": {
        "id": "GYNMIquqzYE-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_dataset_train.shape, z_dataset_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whcJq42SzYCY",
        "outputId": "8305c3ba-0a4a-4592-9a92-9850f347e8d0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((434136, 16), (100000, 16))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z_dataset_train = torch.tensor(z_dataset_train)\n",
        "z_dataset_test = torch.tensor(z_dataset_test)"
      ],
      "metadata": {
        "id": "s0jO-7Ba3Z_2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from models.GAN import *\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "gan_config = {}\n",
        "gan_config['device'] = device\n",
        "gan_config['g_input_dim'] = 4\n",
        "gan_config['g_hidden_dim'] = 8\n",
        "gan_config['g_output_dim'] = 16\n",
        "gan_config['d_hidden_dim'] = 4\n",
        "gan_config['d_output_dim'] = 1\n",
        "gan_config['g_lr'] = 0.001\n",
        "gan_config['d_lr'] = 0.001\n",
        "gan_config['epochs'] = 50\n",
        "gan_config['gamma'] = 0.01\n",
        "\n",
        "gan_config['batch_size'] = 64\n",
        "\n",
        "\n",
        "gan = GANmodel(gan_config)"
      ],
      "metadata": {
        "id": "aPdPlEFYzM0I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gan.train(z_dataset_train, z_dataset_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2dgwFw5zMxh",
        "outputId": "8d0f8bc3-370e-4b3d-a927-dddfa3909833"
      },
      "execution_count": 21,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train dloss: [0.02147960855452767] gloss: [0.011387875150713902], Valid dloss: [0.0213722388446331] gloss: [0.011798070147633552]\n",
            "Epoch 1: Train dloss: [0.02147960855452767, 0.021540217524991318] gloss: [0.011387875150713902, 0.011131908402434783], Valid dloss: [0.0213722388446331, 0.021729616552591324] gloss: [0.011798070147633552, 0.010810973035693169]\n",
            "Epoch 2: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463]\n",
            "Epoch 3: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989]\n",
            "Epoch 4: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341]\n",
            "Epoch 5: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445]\n",
            "Epoch 6: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455]\n",
            "Epoch 7: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232]\n",
            "Epoch 8: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983]\n",
            "Epoch 9: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266]\n",
            "Epoch 10: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099]\n",
            "Epoch 11: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195]\n",
            "Epoch 12: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003]\n",
            "Epoch 13: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402]\n",
            "Epoch 14: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317]\n",
            "Epoch 15: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877]\n",
            "Epoch 16: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635]\n",
            "Epoch 17: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206]\n",
            "Epoch 18: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362]\n",
            "Epoch 19: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506]\n",
            "Epoch 20: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546]\n",
            "Epoch 21: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411]\n",
            "Epoch 22: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281]\n",
            "Epoch 23: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213]\n",
            "Epoch 24: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342]\n",
            "Epoch 25: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804]\n",
            "Epoch 26: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244]\n",
            "Epoch 27: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522]\n",
            "Epoch 28: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344]\n",
            "Epoch 29: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382]\n",
            "Epoch 30: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241]\n",
            "Epoch 31: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694]\n",
            "Epoch 32: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565]\n",
            "Epoch 33: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356]\n",
            "Epoch 34: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197]\n",
            "Epoch 35: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013]\n",
            "Epoch 36: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847]\n",
            "Epoch 37: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997]\n",
            "Epoch 38: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727]\n",
            "Epoch 39: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589]\n",
            "Epoch 40: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106]\n",
            "Epoch 41: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738]\n",
            "Epoch 42: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382]\n",
            "Epoch 43: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404]\n",
            "Epoch 44: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242, 0.021610473990819303] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618, 0.011056124025033759], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504, 0.023471998155117035] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404, 0.009054123223423957]\n",
            "Epoch 45: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242, 0.021610473990819303, 0.02030119434914681] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618, 0.011056124025033759, 0.012690757779346137], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504, 0.023471998155117035, 0.02061139424085617] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404, 0.009054123223423957, 0.011992539878487586]\n",
            "Epoch 46: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242, 0.021610473990819303, 0.02030119434914681, 0.020910955997302715] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618, 0.011056124025033759, 0.012690757779346137, 0.012048748679501519], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504, 0.023471998155117035, 0.02061139424085617, 0.021146565589904784] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404, 0.009054123223423957, 0.011992539878487586, 0.012123941785097122]\n",
            "Epoch 47: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242, 0.021610473990819303, 0.02030119434914681, 0.020910955997302715, 0.021485661752764316] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618, 0.011056124025033759, 0.012690757779346137, 0.012048748679501519, 0.01129716077477362], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504, 0.023471998155117035, 0.02061139424085617, 0.021146565589904784, 0.020263614513874054] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404, 0.009054123223423957, 0.011992539878487586, 0.012123941785097122, 0.01200498907148838]\n",
            "Epoch 48: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242, 0.021610473990819303, 0.02030119434914681, 0.020910955997302715, 0.021485661752764316, 0.021632115626803266] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618, 0.011056124025033759, 0.012690757779346137, 0.012048748679501519, 0.01129716077477362, 0.010990925501041254], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504, 0.023471998155117035, 0.02061139424085617, 0.021146565589904784, 0.020263614513874054, 0.021484915788173676] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404, 0.009054123223423957, 0.011992539878487586, 0.012123941785097122, 0.01200498907148838, 0.010774051996469497]\n",
            "Epoch 49: Train dloss: [0.02147960855452767, 0.021540217524991318, 0.021695112950231757, 0.021454151831429305, 0.021601424745416698, 0.02063115760739618, 0.021555065221572824, 0.02091799796098241, 0.021668000614201335, 0.021322086124685245, 0.02153394039710892, 0.021342129401756494, 0.02142588824281829, 0.02140290816656812, 0.02159922310200809, 0.02111065491719828, 0.02163409036264044, 0.02165716350239767, 0.02150536811976248, 0.021645364836444594, 0.021481445927879213, 0.02163634413656716, 0.02166030982140516, 0.021667000890019227, 0.021418523224280075, 0.021246845393498096, 0.02158817139990477, 0.021314447873811246, 0.021022058041378737, 0.021546009768931947, 0.02152680665606617, 0.021668973432914126, 0.021654392807183644, 0.021599857476308996, 0.021402159577490365, 0.02166932231921854, 0.02148332949450095, 0.021105355851594267, 0.021654483292876586, 0.021666517580712304, 0.020134593397864945, 0.017101366067950527, 0.023314675425604246, 0.020225403752391242, 0.021610473990819303, 0.02030119434914681, 0.020910955997302715, 0.021485661752764316, 0.021632115626803266, 0.021514587163563262] gloss: [0.011387875150713902, 0.011131908402434783, 0.011007439926118941, 0.01178400181966351, 0.010983189933505456, 0.013339423190510652, 0.011236528014884974, 0.011785265195367963, 0.011073938095927896, 0.0115507153310308, 0.011198410757231577, 0.011768293980413829, 0.01367301266701315, 0.0114006300627222, 0.0109391172091823, 0.011821392911099689, 0.010953944150967272, 0.01098573157522926, 0.011264333304337062, 0.011245323652159413, 0.01142535555555079, 0.011000241939624612, 0.011004525151370842, 0.012731478167568559, 0.011618211137144483, 0.011572995312844312, 0.011204667924293659, 0.01154292746822305, 0.012325176500651834, 0.011367094256415623, 0.011009628160521305, 0.01094991822142852, 0.010953023088026252, 0.011041667340323157, 0.012104502804240292, 0.010956511741421112, 0.011362546003242453, 0.012221540365423939, 0.01089882426186159, 0.010904052449195985, 0.012139412237050744, 0.016448555798004916, 0.01265582498600269, 0.012907193844637618, 0.011056124025033759, 0.012690757779346137, 0.012048748679501519, 0.01129716077477362, 0.010990925501041254, 0.010935241422633845], Valid dloss: [0.0213722388446331, 0.021729616552591324, 0.024624178606271745, 0.02151802406668663, 0.020226411002874374, 0.020794052468538285, 0.021853674018383028, 0.023333476781845094, 0.020879102391004562, 0.02044464944958687, 0.021608176658153535, 0.021453955246210098, 0.02256894648194313, 0.02174500736117363, 0.019796084567308424, 0.021756853432655335, 0.02170030955672264, 0.021663997098207474, 0.019780776582956315, 0.02161187252640724, 0.021521540613174437, 0.021695982407331466, 0.0217393703854084, 0.02583985698223114, 0.021675815316438675, 0.02084535904288292, 0.021653181051015855, 0.02242604893088341, 0.020962824008464814, 0.021528978031873702, 0.021505401591062547, 0.021924649102687835, 0.02165905627846718, 0.022027752895355225, 0.02196645670771599, 0.021469041237831117, 0.020500438129901887, 0.021918010643720627, 0.021747958805561067, 0.021828392606973647, 0.014943122450113297, 0.009918749930262566, 0.019460334095954896, 0.01974638308763504, 0.023471998155117035, 0.02061139424085617, 0.021146565589904784, 0.020263614513874054, 0.021484915788173676, 0.013851078276038169] gloss: [0.011798070147633552, 0.010810973035693169, 0.010709012394547463, 0.01074710606276989, 0.011801924332976341, 0.011202705263495445, 0.014406018325686455, 0.012898780608773232, 0.014737130368947983, 0.013931955848932266, 0.011012014915347099, 0.010758196967840195, 0.017281566157341003, 0.010646673982143402, 0.011994876592755317, 0.011045916286110877, 0.010870529225468635, 0.010797102460861206, 0.011382918720245362, 0.010843679735064506, 0.010989570422172546, 0.01156930626988411, 0.010529045128226281, 0.012616356949806213, 0.010821694731712342, 0.011716049141287804, 0.010893728842139244, 0.011035537013411522, 0.01092226823568344, 0.011692519651651382, 0.01176801135957241, 0.010718168870806694, 0.010855726705789565, 0.011198900325894356, 0.011958569126725197, 0.01095022665143013, 0.010581512925624847, 0.010509258710741997, 0.011140819603204727, 0.01046706309914589, 0.017170038088560106, 0.0216047800719738, 0.012525704772472382, 0.013172545060515404, 0.009054123223423957, 0.011992539878487586, 0.012123941785097122, 0.01200498907148838, 0.010774051996469497, 0.016760133656263353]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(gan.d_losses_train, label='d_losses_train')\n",
        "plt.plot(gan.d_losses_val, label='d_losses_val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(gan.g_losses_train, label='g_losses_train')\n",
        "plt.plot(gan.g_losses_val, label='g_losses_val')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "sJ707n5rvzsv",
        "outputId": "425f3e60-0905-4c16-88fd-65a325d47bfa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mKQOKZOS8FGG"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z_SAMPLE_COUNT = 32\n",
        "\n",
        "## TODO: Move into StructuralCausalModel\n",
        "def predict(scm, x, gan, device='cuda'):\n",
        "    x_dup = torch.repeat_interleave(x, Z_SAMPLE_COUNT, dim=0)\n",
        "    z_samples = gan.drawsamples(N=len(x_dup), get_tensor=True)\n",
        "\n",
        "    # x_dup = x_dup.to(device)\n",
        "    # z_samples = z_samples.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        x_feat = model.text_featurizer(x_dup)\n",
        "        y_pred_logits = model.pthetay_xz(x_feat, z_samples).reshape((x.size(0), Z_SAMPLE_COUNT, -1)).mean(dim=1)\n",
        "\n",
        "    return nn.functional.sigmoid(y_pred_logits)\n"
      ],
      "metadata": {
        "id": "nHhAonKs8g3E"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(model, X_train_tensor[:64].cuda(), gan).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXF_D_-1Dels",
        "outputId": "5eba04ef-ae54-4855-d61e-c54f39a00536"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pJPpPnOyOe1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_dataset(model, X_test, gan, batch_size=256):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        test_size = X_test.shape[0]\n",
        "        test_preds = torch.zeros((test_size, ))\n",
        "\n",
        "        for batch in range(0, test_size, batch_size):\n",
        "\n",
        "            start_index = batch\n",
        "            end_index = min(batch + batch_size, test_size)\n",
        "\n",
        "            batch_X = X_test[start_index:end_index].cuda()\n",
        "\n",
        "            output = predict(model, batch_X, gan)\n",
        "\n",
        "            test_preds[start_index:end_index] = output.squeeze(dim=1).cpu()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return test_preds"
      ],
      "metadata": {
        "id": "ucqUaqCHF6Ip"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = predict_dataset(model, X_test_tensor, gan)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkqp4Ol8KqqF",
        "outputId": "6e1c9588-0120-40f1-e38e-160a2ef712c9"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pred_test = predict_dataset(model, X_test_tensor, gan)\n",
        "y_pred = np.round(pred_test.cpu().numpy())\n",
        "\n",
        "print(\"Accuracy: {}\".format(accuracy_score(test_df['label'], y_pred)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sblcpp-hNLA5",
        "outputId": "a6046b0f-40a3-4064-c2aa-fbedc6f7a16b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTwSFXlmNK-3",
        "outputId": "df39c511-1a1c-4d85-d250-2782e599c2e6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vyLCjrvMN9IN"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_yKnBxpBN9D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lfZmrfd2KqoH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "10708_confounder_removal.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}